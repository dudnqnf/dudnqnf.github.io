<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-02-16T18:41:34+09:00</updated><id>http://localhost:4000/</id><title type="html">LeeHyunSik</title><subtitle>blog</subtitle><entry><title type="html">윈도우에서 아타리 설치방법</title><link href="http://localhost:4000/ML_note-%EC%9C%88%EB%8F%84%EC%9A%B0%EC%97%90%EC%84%9C-atari-%EC%84%A4%EC%B9%98%EB%B0%A9%EB%B2%95/" rel="alternate" type="text/html" title="윈도우에서 아타리 설치방법" /><published>2018-01-01T00:00:00+09:00</published><updated>2018-01-01T00:00:00+09:00</updated><id>http://localhost:4000/ML_note#%EC%9C%88%EB%8F%84%EC%9A%B0%EC%97%90%EC%84%9C%20atari%20%EC%84%A4%EC%B9%98%EB%B0%A9%EB%B2%95</id><content type="html" xml:base="http://localhost:4000/ML_note-%EC%9C%88%EB%8F%84%EC%9A%B0%EC%97%90%EC%84%9C-atari-%EC%84%A4%EC%B9%98%EB%B0%A9%EB%B2%95/">&lt;blockquote&gt;
  &lt;p&gt;출처 : http://ishuca.tistory.com/entry/Windows에서-gymatari-설치하기&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;윈도우에서-아타리-설치방법&quot;&gt;윈도우에서 아타리 설치방법&lt;/h1&gt;

&lt;h4 id=&quot;msys2&quot;&gt;MSYS2&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;http://www.msys2.org/&lt;/li&gt;
  &lt;li&gt;CMAKE 등 설치를 위해 필요&lt;/li&gt;
  &lt;li&gt;msys2실행
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;pacman -S base-devel mingw-w64-x86_64-gcc mingw-w64-x86_64-cmake&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;시스템-환경변수-변경&quot;&gt;시스템 환경변수 변경&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;DISPLAY = :0  (새로만들기)&lt;/li&gt;
  &lt;li&gt;PHTYONPATH = c:\path\to\atari-py:$PYTHONPATH  (새로만들기)&lt;/li&gt;
  &lt;li&gt;path
    &lt;ul&gt;
      &lt;li&gt;C:\msys64\mingw64\bin (추가)&lt;/li&gt;
      &lt;li&gt;C:\msys64\usr\bin (추가)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;xming-x-server-설치&quot;&gt;Xming X server 설치&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;https://sourceforge.net/projects/xming/?source=directory&lt;/li&gt;
  &lt;li&gt;linux게임을 GUI로 표현할 때 사용되는듯…&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;atari-설치&quot;&gt;atari 설치&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;git 설치&lt;/li&gt;
  &lt;li&gt;cmd 실행
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;git clone https://github.com/rybskej/atari-py&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;cd atari-py&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;make&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;python setup.py install&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;pip install -U git+https://github.com/Kojoley/atari-py.git&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;오류수정&quot;&gt;오류수정&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;pip install gym[atari]로 하면 오류남&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="Note" /><summary type="html">Machine Learning</summary></entry><entry><title type="html">ML_note #RL-6</title><link href="http://localhost:4000/ML_note-RL-6/" rel="alternate" type="text/html" title="ML_note #RL-6" /><published>2017-12-22T00:00:00+09:00</published><updated>2017-12-22T00:00:00+09:00</updated><id>http://localhost:4000/ML_note#RL-6</id><content type="html" xml:base="http://localhost:4000/ML_note-RL-6/">&lt;blockquote&gt;
  &lt;p&gt;이웅원 외 4명 저 ‘파이썬과 케라스로 배우는 강화학습’ 참고&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;강화학습&quot;&gt;강화학습&lt;/h1&gt;

&lt;h4 id=&quot;목표&quot;&gt;목표&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;브레이크아웃 게임을 DQN과 A3C로 풀기&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;브레이크아웃&quot;&gt;브레이크아웃&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;개요
    &lt;ul&gt;
      &lt;li&gt;아타리 회사에서 개발한 벽부수기 게임(핑퐁)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;브레이크아웃의-문제&quot;&gt;브레이크아웃의 문제&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;이미지를 입력형태로 넣기 어려움 -&amp;gt; CNN&lt;/li&gt;
  &lt;li&gt;픽셀 수가 너무 많음 -&amp;gt; 전처리&lt;/li&gt;
  &lt;li&gt;공의 진행방향을 알려면 프레임을 여러개 입력 받아야 함 -&amp;gt; 프레임스킵&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;브레이크아웃과-cnnconvolution-neural-net&quot;&gt;브레이크아웃과 CNN(Convolution Neural Net.)&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;개요
    &lt;ul&gt;
      &lt;li&gt;영상인식 분야에서 많이 사용되는 신경망&lt;/li&gt;
      &lt;li&gt;필터를 통한 이미지 추상화 후 입력&lt;/li&gt;
      &lt;li&gt;DQN, A3C 알고리즘으로 행동 개선&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;
  def build_model(self):
    # CNN
    model = Sequential()
    model.add(Conv2D(32, (8, 8), strides=(4,4), activation='relu', input_shape=self.state_size))
    model.add(Conv2D(64, (4, 4), strides=(2,2), activation='relu')
    model.add(Conv2D(32, (2, 2), strides=(1,1), activation='relu')
    model.add(Flatten())

    # DQN 신경망
    model.add(Dense(512, activation='relu'))
    model.add(Dense(self.action_size))
    model.summary()
    return model
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;전처리&quot;&gt;전처리&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;이미지를 흑백으로 처리&lt;/li&gt;
  &lt;li&gt;불필요한 공간을 잘라내고 사이즈를 줄임&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;프레임-스킵&quot;&gt;프레임 스킵&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;여러 프레임을 놓고 일정 간격으로 프레임 선택&lt;/li&gt;
  &lt;li&gt;‘BreakoutDeterministic-v4’부터는 자동지원&lt;/li&gt;
  &lt;li&gt;프레임 입력은 [1,2,3,4], [2,3,4,5], … 식으로 겹쳐서 입력&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;브레이크아웃과-dqn&quot;&gt;브레이크아웃과 DQN&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;개요
    &lt;ul&gt;
      &lt;li&gt;state = 4개의 과거 프레임을 겹친것(depth = 4)&lt;/li&gt;
      &lt;li&gt;방향 = 왼쪽, 오른쪽, 정지&lt;/li&gt;
      &lt;li&gt;q함수 = 각 방향으로 진행했을 경우 받을 보상의 합(동일)&lt;/li&gt;
      &lt;li&gt;가장 높은 q함수를 선택하여 정책 개선(가치기반)&lt;/li&gt;
      &lt;li&gt;cost는 기존대로 (정답 Q함수 - 예측 Q함수)^2&lt;/li&gt;
      &lt;li&gt;경험리플레이를 사용&lt;/li&gt;
      &lt;li&gt;행동 -&amp;gt; 행동 -&amp;gt; 행동 -&amp;gt; … -&amp;gt; 행동 -&amp;gt; 개선 -&amp;gt; 행동 -&amp;gt; 개선 -&amp;gt; …&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;경험리플레이
    &lt;ul&gt;
      &lt;li&gt;각각의 상태를 저장해 놨다가 쌓이면 랜덤으로 꺼내서 개선&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;신경망(model, target_model)
    &lt;ul&gt;
      &lt;li&gt;현재 상태에 대한 q함수 예측&lt;/li&gt;
      &lt;li&gt;입력 : 현재 상태에 대한 정보&lt;/li&gt;
      &lt;li&gt;&lt;del&gt;심층신경망(입력층1, 은닉층2, 출력층1:linear)&lt;/del&gt;&lt;/li&gt;
      &lt;li&gt;출력 : q함수 테이블&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;행동 및 개선
    &lt;ul&gt;
      &lt;li&gt;현재 상태(s)를 입력으로 심층신경망을 통하여 q함수 테이블을 만들고 그 중 최고값을 행동으로 선택(a) (가끔식 모험)&lt;/li&gt;
      &lt;li&gt;행동에 따른 보상(r)과 다음 상태(s’)를 추출&lt;/li&gt;
      &lt;li&gt;s, a, r, s’ 저장&lt;/li&gt;
      &lt;li&gt;s’으로 이동&lt;/li&gt;
      &lt;li&gt;일정 이상의 데이터가 쌓일 때까지 계속 행동&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;개선
    &lt;ul&gt;
      &lt;li&gt;일정치 이상 데이터가 쌓였다면 개선 시작(이 다음부터는 행동 한 번에 개선 한 번)&lt;/li&gt;
      &lt;li&gt;데이터들 중 무작위로 추출&lt;/li&gt;
      &lt;li&gt;(s,a)로 q추출(model 사용) / r / (s)로 q’의 최고값 추출(target_model 사용)&lt;/li&gt;
      &lt;li&gt;딥살사와 다른점은 action과 상관없이 최고 q함수 선택&lt;/li&gt;
      &lt;li&gt;s 입력, (r + 감가율  * q’) 정답으로 신경망 개선(model)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;추가&quot;&gt;추가&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;후버로스 그래프
    &lt;ul&gt;
      &lt;li&gt;기존 오류함수(2차함수)보다 더 안정적&lt;/li&gt;
      &lt;li&gt;-1 ~ 1 사이의 값은 2차함수를 따르고 나머지 부분은 1차함수 그래프&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;브레이크아웃과-a3c&quot;&gt;브레이크아웃과 A3C&lt;/h4&gt;</content><author><name></name></author><category term="Note" /><summary type="html">Machine Learning</summary></entry><entry><title type="html">ML_note #RL-5</title><link href="http://localhost:4000/ML_note-RL-5/" rel="alternate" type="text/html" title="ML_note #RL-5" /><published>2017-12-22T00:00:00+09:00</published><updated>2017-12-22T00:00:00+09:00</updated><id>http://localhost:4000/ML_note#RL-5</id><content type="html" xml:base="http://localhost:4000/ML_note-RL-5/">&lt;blockquote&gt;
  &lt;p&gt;이웅원 외 4명 저 ‘파이썬과 케라스로 배우는 강화학습’ 참고&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;강화학습&quot;&gt;강화학습&lt;/h1&gt;

&lt;h4 id=&quot;목표&quot;&gt;목표&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;카트폴 문제를 DQN과 AC로 풀기&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;카트폴-문제&quot;&gt;카트폴 문제&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;개요
    &lt;ul&gt;
      &lt;li&gt;OpenAI GYM에 있는 예제&lt;/li&gt;
      &lt;li&gt;카트 위에 달려있는 폴이 한쪽으로 쓰러지지 않게 카트를 계속해서 움직이는 게임&lt;/li&gt;
      &lt;li&gt;에이전트 상태 = [x, x’, θ, θ’] &lt;br /&gt;(x : 카트 위치, x’ : 카트 속도, θ : 폴 기운 각도, θ’ : 폴 각속도)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;해결전략 : DQN, AC&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;딥큐러닝dqn&quot;&gt;딥큐러닝(DQN)&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;개요
    &lt;ul&gt;
      &lt;li&gt;Q-learning + DNN&lt;/li&gt;
      &lt;li&gt;인공신경망을 통해 Q함수 근사&lt;/li&gt;
      &lt;li&gt;딥살사와 동일하나 행동한 곳이 아닌 최대 Q함수로의 update ( 살사와 Q-learning의 차이 )&lt;/li&gt;
      &lt;li&gt;경험리플레이를 사용&lt;/li&gt;
      &lt;li&gt;행동 -&amp;gt; 행동 -&amp;gt; 행동 -&amp;gt; … -&amp;gt; 행동 -&amp;gt; 개선 -&amp;gt; 행동 -&amp;gt; 개선 -&amp;gt; …&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;경험리플레이
    &lt;ul&gt;
      &lt;li&gt;이동경로를 기억해 놨다가 쌓이면 랜덤으로 꺼내서 개선&lt;/li&gt;
      &lt;li&gt;딥살사는 각각의 위치에서 움직일 action이 정해짐(action 방향으로의 정책 update만 되어야 함 - 온폴리시)&lt;/li&gt;
      &lt;li&gt;Q-learning은 정책update와 action이 따로(최고 q함수로 정책 update을 하면 됨 - 오프폴리시)&lt;/li&gt;
      &lt;li&gt;action과 관련성이 없는 Q-learning에서 경험리플레이의 사용이 가능&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;만약 딥살사에서 경험리플레이를 사용한다면?&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;경로의 마지막부터 거꾸로 학습이 된다고 할 때 잘못된 모험을 하여 q함수값이 감소한다면&lt;/li&gt;
    &lt;li&gt;그 경로 위의 모든 노드는 q함수값이 감소하게 될 것!&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;심층신경망(model, target_model)
    &lt;ul&gt;
      &lt;li&gt;현재 상태에 대한 Q함수 예측&lt;/li&gt;
      &lt;li&gt;입력 : 현재 상태에 대한 정보&lt;/li&gt;
      &lt;li&gt;심층신경망(입력층1, 은닉층2, 출력층1:linear)&lt;/li&gt;
      &lt;li&gt;출력 : q함수 테이블&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;행동
    &lt;ul&gt;
      &lt;li&gt;현재 상태(s)를 입력으로 심층신경망을 통하여 q함수 테이블을 만들고 그 중 최고값을 행동으로 선택(a) (가끔식 모험)&lt;/li&gt;
      &lt;li&gt;행동에 따른 보상(r)과 다음 상태(s’)를 추출&lt;/li&gt;
      &lt;li&gt;s, a, r, s’ 저장&lt;/li&gt;
      &lt;li&gt;s’으로 이동&lt;/li&gt;
      &lt;li&gt;일정 이상의 데이터가 쌓일 때까지 계속 행동&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;개선
    &lt;ul&gt;
      &lt;li&gt;일정치 이상 데이터가 쌓였다면 개선 시작(이 다음부터는 행동 한 번에 개선 한 번)&lt;/li&gt;
      &lt;li&gt;데이터들 중 무작위로 추출&lt;/li&gt;
      &lt;li&gt;(s,a)로 q추출(model 사용) / r / (s)로 q’의 최고값 추출(target_model 사용)&lt;/li&gt;
      &lt;li&gt;딥살사와 다른점은 action과 상관없이 최고 q함수 선택&lt;/li&gt;
      &lt;li&gt;s 입력, (r + 감가율 * q’) 정답으로 신경망 개선(model)&lt;/li&gt;
      &lt;li&gt;한 번의 episode가 끝나면 target_model을 현재 model로 update&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;DQN에서는 target_model과 model 두 가지의 신경망을 이용한다&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;액터-크리틱ac&quot;&gt;액터-크리틱(AC)&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;개요
    &lt;ul&gt;
      &lt;li&gt;폴리시그레이디언트는 반환값(감가율이 계산된 보상값들)을 사용하기에 목표지점까지 도달 후 개선 가능&lt;/li&gt;
      &lt;li&gt;액터-크리틱은 반환값대신 Q함수를 사용하여 스텝마다 개선이 가능&lt;/li&gt;
      &lt;li&gt;Q함수를 근사하는 인공신경망을 하나 더 만드는 것(=가치신경망)&lt;/li&gt;
      &lt;li&gt;가치신경망에서 현재의 가치(v)와 다음의 가치(v’)를 예측&lt;/li&gt;
      &lt;li&gt;가치신경망은 cost = ( (R + ᵧv’) - v )^2&lt;/li&gt;
      &lt;li&gt;정책신경망은 기존 반환값 위치를 어드벤티지 함수로 대체&lt;/li&gt;
      &lt;li&gt;큐함수로 대체하면 변화 정도가 너무 큼 -&amp;gt; 큐함수 - 가치함수(베이스라인) 로 대체 -&amp;gt; 어드벤티지 함수&lt;/li&gt;
      &lt;li&gt;어드벤티지 함수 = (R + ᵧv’) - v&lt;/li&gt;
      &lt;li&gt;정책신경망 = (log(정책))’ * 어드벤티지 함수&lt;/li&gt;
      &lt;li&gt;Actor-Critic(AC) = Advantage Actor-Critic(A2C) 라고도 부름&lt;/li&gt;
      &lt;li&gt;행동 -&amp;gt; 개선 -&amp;gt; 행동 -&amp;gt; 개선 -&amp;gt; 행동 -&amp;gt; …&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;신경망1(정책신경망)
    &lt;ul&gt;
      &lt;li&gt;현재 상태에 대한 행동확률 예측&lt;/li&gt;
      &lt;li&gt;입력 : 현재 상태에 대한 정보&lt;/li&gt;
      &lt;li&gt;출력 : 행동확률 테이블(4)&lt;/li&gt;
      &lt;li&gt;cost를 계산하는 식이 따로 존재(log …) =&amp;gt; keras에서 loss모델 만들어야 됨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;신경망2(가치신경망)
    &lt;ul&gt;
      &lt;li&gt;현재 상태에 대한 가치함수 예측&lt;/li&gt;
      &lt;li&gt;입력 : 현재 상태에 대한 정보&lt;/li&gt;
      &lt;li&gt;심층신경망(입력층1, 은닉층2, 출력층1:linear)&lt;/li&gt;
      &lt;li&gt;출력 : 가치함수(1)&lt;/li&gt;
      &lt;li&gt;정답이 존재하므로 cost = (정답 - 예측)^2 =&amp;gt; keras에서 loss모델 안만들어도 됨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;행동
    &lt;ul&gt;
      &lt;li&gt;현재 상태(s)를 입력으로 정책신경망을 통하여 행동확률 테이블을 만들고 확률적으로 선택하여 이동(룰렛휠)&lt;/li&gt;
      &lt;li&gt;행동에 따른 보상(r)과 다음 상태(s’)를 추출&lt;/li&gt;
      &lt;li&gt;s’으로 이동&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;개선
    &lt;ul&gt;
      &lt;li&gt;현재 상태(s)와 다음 상태(s’)을 입력으로 가치신경망을 통하여 현재 상태의 가치(v)와 다음 상태의 가치(v’)을 예측&lt;/li&gt;
      &lt;li&gt;정책신경망
        &lt;ul&gt;
          &lt;li&gt;어드벤티지 함수 생성 (= (R + ᵧv’) - v)&lt;/li&gt;
          &lt;li&gt;현재 상태(s) 입력, ((log(정책))’ * 어드벤티지 함수)가 cost로 신경망 개선&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;가치신경망
        &lt;ul&gt;
          &lt;li&gt;target = (R + ᵧv’)&lt;/li&gt;
          &lt;li&gt;현재 상태(s) 입력, target을 정답으로 신경망 개선&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="Note" /><summary type="html">Machine Learning</summary></entry><entry><title type="html">ML_note #RL-4</title><link href="http://localhost:4000/ML_note-RL-4/" rel="alternate" type="text/html" title="ML_note #RL-4" /><published>2017-12-22T00:00:00+09:00</published><updated>2017-12-22T00:00:00+09:00</updated><id>http://localhost:4000/ML_note#RL-4</id><content type="html" xml:base="http://localhost:4000/ML_note-RL-4/">&lt;blockquote&gt;
  &lt;p&gt;이웅원 외 4명 저 ‘파이썬과 케라스로 배우는 강화학습’ 참고&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;강화학습&quot;&gt;강화학습&lt;/h1&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;남은-문제점&quot;&gt;남은 문제점&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;계산복잡도가 높다는 문제&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;목표&quot;&gt;목표&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;NN을 이용한 계산복잡도 문제 해결&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;그리드월드와-근사함수&quot;&gt;그리드월드와 근사함수&lt;/h2&gt;
&lt;h4 id=&quot;계산복잡도가-높다는-문제&quot;&gt;계산복잡도가 높다는 문제&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;차원의 수, 노드의 수가 많아지면 Q함수를 가지고 있기 너무 큼&lt;/li&gt;
  &lt;li&gt;10 * 10 그리드월드라 할 때 행동이 4가지이기 때문에 10 * 10 * 4의 Q함수 테이블을 연산에 이용해야 함&lt;/li&gt;
  &lt;li&gt;근사함수를 통한 가치함수의 매개변수화를 통해 해결가능&lt;/li&gt;
  &lt;li&gt;SARSA -&amp;gt; 딥살사&lt;/li&gt;
  &lt;li&gt;몬테카를로 -&amp;gt; 폴리시 그레이디언트&lt;/li&gt;
  &lt;li&gt;Q-learning -&amp;gt; DQN(Deep Q-network)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;인공신경망nn과-케라스&quot;&gt;인공신경망(NN)과 케라스&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;인공신경망에 대한 설명은 생락&lt;/li&gt;
  &lt;li&gt;케라스에서 인공신경망 구현 코드&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;
from keras.layers import Dense
from keras.models import Sequential

x_train = []
y_train = []

model = Sequential()
model.add(Dense(12, input_dim=5, activation='sigmoid'))
model.add(Dense(30, activation='sigmoid'))
model.add(Dense(1, activation='RMSProp'))
model.compile(loss='mse', optimizer='RMSProp')

model.fit(x_train, y_train, batch_size=32, epoch=1)

&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;딥살사-이론&quot;&gt;딥살사 이론&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;개요
    &lt;ul&gt;
      &lt;li&gt;살사 + DNN&lt;/li&gt;
      &lt;li&gt;살사이론의 q함수 테이블을 근사함수를 만들어 저장&lt;/li&gt;
      &lt;li&gt;행동시 지정 위치의 q함수들만 예측&lt;/li&gt;
      &lt;li&gt;개선은 다음 지점의 q함수를 정답으로 하여 인공신경망 개선교육&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;심층신경망
    &lt;ul&gt;
      &lt;li&gt;현재 상태에 대한 Q함수 예측&lt;/li&gt;
      &lt;li&gt;입력 : 현재 상태에 대한 정보&lt;/li&gt;
      &lt;li&gt;심층신경망(입력층1, 은닉층2, 출력층1:linear)&lt;/li&gt;
      &lt;li&gt;출력 : q함수 테이블&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;행동
    &lt;ul&gt;
      &lt;li&gt;현재 상태(s)를 입력으로 심층신경망을 통하여 q함수 테이블을 생성&lt;/li&gt;
      &lt;li&gt;q함수 테이블에서 최고값으로 행동으로 선택(a)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;개선
    &lt;ul&gt;
      &lt;li&gt;심층신경망의 가중치를 개선 ( = Q함수 개선 )&lt;/li&gt;
      &lt;li&gt;살사에서의 오류값을 이용&lt;/li&gt;
      &lt;li&gt;오차 = (정답 – 예측)&lt;/li&gt;
      &lt;li&gt;오차 = R + γQ(S2, A2) – Q(S1, S2)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;정책기반 강화학습 vs 가치기반 강화학습&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;정책기반 : 정책에 따라 행동(정책평가가 이루어지지 않으면 최고가치가 존재해도 따라가지 않음)&lt;/li&gt;
    &lt;li&gt;가치기반 : 최고가치가 최고정책으라는 가정하에 행동&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;폴리시-그레이디언트&quot;&gt;폴리시 그레이디언트&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;개요
    &lt;ul&gt;
      &lt;li&gt;몬테카를로 + DNN&lt;/li&gt;
      &lt;li&gt;행동 -&amp;gt; 행동 -&amp;gt; 행동 -&amp;gt; … -&amp;gt; 개선&lt;/li&gt;
      &lt;li&gt;q함수 근사가 아닌 정책 근사로 행동 및 개선&lt;/li&gt;
      &lt;li&gt;탐욕정책이 아닌 각 방향으로 갈 확률 근사&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;심층신경망
    &lt;ul&gt;
      &lt;li&gt;현재 상태에 대한 각 방향의 정책 예측&lt;/li&gt;
      &lt;li&gt;입력 : 현재 상태에 대한 정보&lt;/li&gt;
      &lt;li&gt;심층신경망(입력층1, 은닉층2, 출력층1:sofrmax)&lt;/li&gt;
      &lt;li&gt;출력 : 각 행동을 할 확률&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;행동
    &lt;ul&gt;
      &lt;li&gt;현재 상태(s)를 입력으로 심층신경망을 통하여 각 방향으로 갈 확률 계산&lt;/li&gt;
      &lt;li&gt;확률 기반으로 하나 선택(룰렛휠)&lt;/li&gt;
      &lt;li&gt;끝날 때까지 계속 이동&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;개선
    &lt;ul&gt;
      &lt;li&gt;지나온 경로 각각의 cost 계산&lt;/li&gt;
      &lt;li&gt;s 입력, cost로 개선&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;문제
      &lt;ul&gt;
        &lt;li&gt;현재 정책에 대한 정답값이 없음&lt;/li&gt;
        &lt;li&gt;cost의 계산이 불가&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;해결책
      &lt;ul&gt;
        &lt;li&gt;각 위치의 가치함수(감가율이 계산된 보상값들의 합) = A&lt;/li&gt;
        &lt;li&gt;각 위치에서의 행동에 해당하는 정책값 = B&lt;/li&gt;
        &lt;li&gt;cost = - log(B) * A&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;왜냐하면…
      &lt;ul&gt;
        &lt;li&gt;정책신경망 업데이트 방식 : Q = Q + 학습속도*(cost)’&lt;/li&gt;
        &lt;li&gt;cost = 정책 * q함수 (Richard Sutton의 논문)&lt;/li&gt;
        &lt;li&gt;cost’ = 정책’ * q함수&lt;/li&gt;
        &lt;li&gt;cost’ = (정책’/정책) * 가치함수 =&amp;gt;  because. 가치함수 = 정책 ** q함수 (기존식)&lt;/li&gt;
        &lt;li&gt;cost’ = (log(정책))’ * 가치함수&lt;/li&gt;
        &lt;li&gt;cost’ = (log(정책))’ * (감가율이 계산된 보상의 합)&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;계산복잡도 문제 해결&lt;/code&gt;&lt;/p&gt;</content><author><name></name></author><category term="Note" /><summary type="html">Machine Learning</summary></entry><entry><title type="html">ML_note #RL-3</title><link href="http://localhost:4000/ML_note-RL-3/" rel="alternate" type="text/html" title="ML_note #RL-3" /><published>2017-12-22T00:00:00+09:00</published><updated>2017-12-22T00:00:00+09:00</updated><id>http://localhost:4000/ML_note#RL-3</id><content type="html" xml:base="http://localhost:4000/ML_note-RL-3/">&lt;blockquote&gt;
  &lt;p&gt;이웅원 외 4명 저 ‘파이썬과 케라스로 배우는 강화학습’ 참고&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;강화학습&quot;&gt;강화학습&lt;/h1&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;문제점&quot;&gt;문제점&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;환경에 대한 정보부족 문제&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;계산복잡도가 높다는 문제&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;목표&quot;&gt;목표&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;새로운 모델을 통한 환경에 대한 정보부족 문제 해결&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;그리드월드와-큐러닝&quot;&gt;그리드월드와 큐러닝&lt;/h2&gt;
&lt;h4 id=&quot;환경에-대한-정보가-없는-문제&quot;&gt;환경에 대한 정보가 없는 문제&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;모든 곳의 보상값을 아는 것 vs 보상값을 모르는 것&lt;/li&gt;
  &lt;li&gt;행동하여 간 곳만 개선(실제 문제)&lt;/li&gt;
  &lt;li&gt;행동 -&amp;gt; 개선 -&amp;gt; 행동 -&amp;gt; 개션 -&amp;gt; …&lt;/li&gt;
  &lt;li&gt;몬테카를로 예측, 시간차 예측&lt;/li&gt;
  &lt;li&gt;SARSA, Q-learning&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;몬테카를로-예측&quot;&gt;몬테카를로 예측&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;알파고 작동원리&lt;/li&gt;
  &lt;li&gt;많은 샘플링을 통하여 실제 값의 근사치를 찾아나가는 방법&lt;/li&gt;
  &lt;li&gt;한 번의 경로가 하나의 샘플링이 되어 그 경로의 값들을 개선&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;시간차-예측&quot;&gt;시간차 예측&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;몬테카를로는 실시간이 아님&lt;/li&gt;
  &lt;li&gt;한 번의 스텝으로도 가치함수를 update하는 방법&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;sarsa&quot;&gt;SARSA&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;S = state, A = action, R = reward, S = state, A = action&lt;/li&gt;
  &lt;li&gt;시간차 예측을 통한 학습 알고리즘&lt;/li&gt;
  &lt;li&gt;정책평가 + 정책발전 = 개선(Q함수 update)&lt;/li&gt;
  &lt;li&gt;행동 -&amp;gt; 개선 -&amp;gt; 행동 -&amp;gt; …&lt;/li&gt;
  &lt;li&gt;행동(입실론 탐욕)
    &lt;ul&gt;
      &lt;li&gt;행동은 기본적으로 가장 높은 Q함수로 진행&lt;/li&gt;
      &lt;li&gt;입실론의 확률로 랜덤 노드를 선택&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;개선
    &lt;ul&gt;
      &lt;li&gt;행동한 방향으로 Q함수 update&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;입실론 탐욕정책으로 행동하고 그 방향으로 발전(온폴리시)&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;원래는 Q = 보상 + 감가율 * 가치함수&lt;/li&gt;
    &lt;li&gt;탐욕정책으로 인하여 가장 높은 Q함수 = 가치함수&lt;/li&gt;
    &lt;li&gt;Q = 다음 노드의 보상 + 감가율 * Q’&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;q-learning&quot;&gt;Q-learning&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;행동
    &lt;ul&gt;
      &lt;li&gt;SARSA와 동일하게 탐욕정책&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;개선
    &lt;ul&gt;
      &lt;li&gt;SARSA와 동일하지만 만약 에이전트가 모험을 선택했다가 더 안좋은 상황이 생겼다면?&lt;/li&gt;
      &lt;li&gt;행동한 곳 기준으로 개선하는 것이 아니라 무조건 가장높은 Q’값 기준으로 개선&lt;/li&gt;
      &lt;li&gt;SARSA의 문제점 보완&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;환경문제 해결&lt;/code&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;남은-문제점&quot;&gt;남은 문제점&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;계산복잡도가 높다는 문제&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;</content><author><name></name></author><category term="Note" /><summary type="html">Machine Learning</summary></entry><entry><title type="html">ML_note #RL-2</title><link href="http://localhost:4000/ML_note-RL-2/" rel="alternate" type="text/html" title="ML_note #RL-2" /><published>2017-12-22T00:00:00+09:00</published><updated>2017-12-22T00:00:00+09:00</updated><id>http://localhost:4000/ML_note#RL-2</id><content type="html" xml:base="http://localhost:4000/ML_note-RL-2/">&lt;blockquote&gt;
  &lt;p&gt;이웅원 외 4명 저 ‘파이썬과 케라스로 배우는 강화학습’ 참고&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;강화학습&quot;&gt;강화학습&lt;/h1&gt;

&lt;h4 id=&quot;목표&quot;&gt;목표&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;강화학습의 기본 모델 학습&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;그리드-월드와-다이내믹-프로그래밍&quot;&gt;그리드 월드와 다이내믹 프로그래밍&lt;/h2&gt;
&lt;h4 id=&quot;그리드월드&quot;&gt;그리드월드&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;n * n의 격자 공간&lt;/li&gt;
  &lt;li&gt;이전에 가정했던 5 * 5 격자공간을 가정&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;동적-계획법다이내믹-프로그래밍&quot;&gt;동적 계획법(다이내믹 프로그래밍)&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;정의
    &lt;ul&gt;
      &lt;li&gt;리처드 벨만에 의해서 만들어진 최적화 방정식&lt;/li&gt;
      &lt;li&gt;나무위키 : https://namu.wiki/w/동적%20계획법&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;예제
    &lt;ul&gt;
      &lt;li&gt;https://www.acmicpc.net/problem/1003&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;동적-계획법을-통한-강화학습&quot;&gt;동적 계획법을 통한 강화학습&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;가치(Q함수) update 시에 사용&lt;/li&gt;
  &lt;li&gt;V 	=  E[𝑹_𝟏  + γ 𝑹_𝟐 + γ^𝟐 𝑹_𝟑   + … + γ^(𝒏−𝟏) 𝑹_𝒏]&lt;br /&gt;
  	=   E[𝑹_𝟏 + γV’]&lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;q&lt;em&gt;(s)  	= E[R + γ max q&lt;/em&gt; (𝑺_(𝒕+𝟏), a’)&lt;/td&gt;
          &lt;td&gt;𝑺_𝒕 = s, 𝑨_𝒕=𝒂 ]&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;벨만 기대 방정식 -&amp;gt; 정책 이터레이션&lt;/li&gt;
  &lt;li&gt;벨만 최적 방정식 -&amp;gt; 가치 이터레이션&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;정책-평가와-정책-발전&quot;&gt;정책 평가와 정책 발전&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;정책평가 = Q함수 값들을 이용해 현재 노드 가치 update&lt;/li&gt;
  &lt;li&gt;정책발전(탐욕정책) = Q함수값들을 이용해 현재 노드의 정책 update&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;정책-이터레이션&quot;&gt;정책 이터레이션&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;평가 -&amp;gt; 평가 -&amp;gt; 발전 -&amp;gt; 평가 -&amp;gt; … -&amp;gt; 평가 -&amp;gt; &lt;strong&gt;행동&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;평가시 이전 정책을 토대로 평가&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;가치-이터레이션&quot;&gt;가치 이터레이션&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;정책 발전이 존재하지 않음&lt;/li&gt;
  &lt;li&gt;평가 -&amp;gt; 평가 -&amp;gt;  … -&amp;gt; 평가 -&amp;gt; &lt;strong&gt;행동&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;평가시 정책과 상관없이 최대Q함수로 가치 update&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;문제&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;Q : 정책 이터레이션도 탐욕정책을 취하기에 가장 큰 Q함수를 가진 방향의 정책만을 취하게 되는데 그러면 가치 이터레이션과 동일하다?&lt;/li&gt;
    &lt;li&gt;A : 정책 이터레이션은 정책이 명확히 명시되어 있어 그 정책에 따라서만 발전한다.
하지만 가치 이터레이션은 정책이 명확히 명시되어 있지 않고 최대 Q함수로만 발전된다.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;문제점&quot;&gt;문제점&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;환경에 대한 정보부족 문제&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;계산복잡도가 높다는 문제&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;</content><author><name></name></author><category term="Note" /><summary type="html">Machine Learning</summary></entry><entry><title type="html">ML_note #RL-1</title><link href="http://localhost:4000/ML_note-RL-1/" rel="alternate" type="text/html" title="ML_note #RL-1" /><published>2017-12-22T00:00:00+09:00</published><updated>2017-12-22T00:00:00+09:00</updated><id>http://localhost:4000/ML_note#RL-1</id><content type="html" xml:base="http://localhost:4000/ML_note-RL-1/">&lt;blockquote&gt;
  &lt;p&gt;이웅원 외 4명 저 ‘파이썬과 케라스로 배우는 강화학습’ 참고&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;강화학습&quot;&gt;강화학습&lt;/h1&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;강화학습이란&quot;&gt;강화학습이란&lt;/h2&gt;
&lt;h4 id=&quot;개요&quot;&gt;개요&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;시행착오를 통하여 학습하는 알고리즘&lt;/li&gt;
  &lt;li&gt;실제상황
    &lt;ul&gt;
      &lt;li&gt;상황이 주어짐 (자전거를 타는 상황)&lt;/li&gt;
      &lt;li&gt;에이전트가 행동 (오른쪽으로 핸들 돌리기)&lt;/li&gt;
      &lt;li&gt;환경이 보상 (넘어짐 / 안넘어짐)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;적용가능한-문제들&quot;&gt;적용가능한 문제들&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;경로 찾기, 스케줄링&lt;/li&gt;
  &lt;li&gt;미로찾기, Frozen Lake, TSP …&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;기본-모델&quot;&gt;기본 모델&lt;/h2&gt;
&lt;h4 id=&quot;순차적-행동-결정-문제&quot;&gt;순차적 행동 결정 문제&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;순차적으로 행동을 결정해야하는 문제들&lt;/li&gt;
  &lt;li&gt;5 * 5 격자공간에서 이동 최단거리를 탐색 문제로 가정
( (0, 0)에서 시작, (4, 4)에서 종료 )&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;순차적-행동-결정-문제의-구성-요소&quot;&gt;순차적 행동 결정 문제의 구성 요소&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;상태(state) : &amp;lt;/tab&amp;gt;현재 내가 어디에 있는지&lt;/li&gt;
  &lt;li&gt;행동(action) : 어디로 이동할지&lt;/li&gt;
  &lt;li&gt;보상(reward) : 행동 했을 때 좋은 행동인지 안 좋은 행동인지&lt;/li&gt;
  &lt;li&gt;정책(policy) : 각각의 상황에서 어디로 이동할 지에 대한 표지판&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;mdp와-벨만-방정식&quot;&gt;MDP와 벨만 방정식&lt;/h2&gt;
&lt;h4 id=&quot;mdpmarkov-decision-process&quot;&gt;MDP(Markov Decision Process)&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;순차적 행동결정 문제를 수학적 정의하여 프로세스를 설명&lt;/li&gt;
  &lt;li&gt;상태 / 행동 / 보상 / 상태 변환 확률 / 정책 으로 구성&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;mdp-구성-요소&quot;&gt;MDP 구성 요소&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;상태(S) = (1, 1) : (1, 1)지점에 존재&lt;/li&gt;
  &lt;li&gt;행동(A) = (1,0,0,0) : 위로 움직임&lt;/li&gt;
  &lt;li&gt;보상함수(R)
    &lt;ul&gt;
      &lt;li&gt;\({e}^{i\pi}+1=0\)&lt;/li&gt;
      &lt;li&gt;
        &lt;table&gt;
          &lt;tbody&gt;
            &lt;tr&gt;
              &lt;td&gt;$R = E[R&lt;/td&gt;
              &lt;td&gt;S, A]$&lt;/td&gt;
            &lt;/tr&gt;
          &lt;/tbody&gt;
        &lt;/table&gt;
      &lt;/li&gt;
      &lt;li&gt;(1,1) 지점에서 위로 움직였을 때 받을 보상(E = 기대)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;상태 변환 확률(P)
    &lt;ul&gt;
      &lt;li&gt;
        &lt;table&gt;
          &lt;tbody&gt;
            &lt;tr&gt;
              &lt;td&gt;P = P[S = s’&lt;/td&gt;
              &lt;td&gt;S = s, A = a] : (1, 1) 지점에서 움직일지 말지&lt;/td&gt;
            &lt;/tr&gt;
          &lt;/tbody&gt;
        &lt;/table&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;정책
    &lt;ul&gt;
      &lt;li&gt;
        &lt;table&gt;
          &lt;tbody&gt;
            &lt;tr&gt;
              &lt;td&gt;π(a&lt;/td&gt;
              &lt;td&gt;s) = P[A = a&lt;/td&gt;
              &lt;td&gt;S = s] : (1,1) 위치에서 어디로 이동해야 가장 좋을지&lt;/td&gt;
            &lt;/tr&gt;
          &lt;/tbody&gt;
        &lt;/table&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;mdp-외-문제를-풀기위해-필요한-개념&quot;&gt;MDP 외 문제를 풀기위해 필요한 개념&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;감가율( γ )&lt;/li&gt;
  &lt;li&gt;모든 보상은 감가율이 계산되어 가치로 환산됨&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;감가율을 고려한 미래보상의 현재 가치 = γ^(𝒌−𝟏) 𝑹_𝒌&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;반환값 (G)
    &lt;ul&gt;
      &lt;li&gt;감가율이 계산된 보삼들의 합&lt;/li&gt;
      &lt;li&gt;𝑹_𝟏  + γ𝑹_𝟐 + γ^𝟐 𝑹_𝟑 + … + γ^(𝒏−𝟏) 𝑹_𝒏&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;가치함수(V)&lt;/li&gt;
  &lt;li&gt;각각의 노드들이 가지고 있는 가치&lt;/li&gt;
  &lt;li&gt;이후에 받을 감가율이 계산된 보상들의 합&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;V 	=  E[G | S = s] &lt;br /&gt;
  =  E[𝑹_𝟏  + γ𝑹_𝟐 + “γ” ^𝟐 𝑹_𝟑   + … + “γ” ^(𝒏−𝟏) 𝑹_𝒏]&lt;br /&gt;
  =   E[𝑹_𝟏 + γV’]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Q함수
    &lt;ul&gt;
      &lt;li&gt;행동 가치함수 (방향이 정해진 가치함수)&lt;/li&gt;
      &lt;li&gt;한 상태에서 네 방향으로의 진행이 가능하기 때문에 가치함수를 구하려면 네 방향의 보상과 가치함수를 고려해야 함&lt;/li&gt;
      &lt;li&gt;오른쪽의 Q함수 = (오른쪽의 보상 + γ * 오른쪽 노드의 가치함수)&lt;/li&gt;
      &lt;li&gt;오른쪽의 Q함수 = (오른쪽의 보상 + γ * 오른쪽 노드의 다음행동 Q함수)&lt;/li&gt;
      &lt;li&gt;계산 가능 형태의 가치함수(V)&lt;br /&gt;
= 각 방향의 정책 * 각 방향의 Q함수 &lt;br /&gt;
=  ∑π(a|s) q(s, a)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;벨만-방정식&quot;&gt;벨만 방정식&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;가치를 계산하는 방정식&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;벨만-기대-방정식&quot;&gt;벨만 기대 방정식&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;정책에 따라서 가치를 판단 ( 정책은 항상 고정되어 있고 정책 발전시에만 변경됨 )&lt;/li&gt;
  &lt;li&gt;가치함수 = 각 방향의 정책 * 각 방향의 (보상 + 감가율 * 다음 상태의 가치함수)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;벨만-최적-방정식&quot;&gt;벨만 최적 방정식&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;가장 높은 Q함수를 가진 노드만을 선택하여 가치함수 계산 ( 정책과 상관없이 )&lt;/li&gt;
  &lt;li&gt;가치함수 = 보상 + 감가율 * 최고 가치를 가진 방향의 가치함수&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="Note" /><summary type="html">Machine Learning</summary></entry><entry><title type="html">ML_note #RNN-1</title><link href="http://localhost:4000/ML_note-RNN-1(%EB%AF%B8%EC%99%84%EC%84%B1)/" rel="alternate" type="text/html" title="ML_note #RNN-1" /><published>2017-12-22T00:00:00+09:00</published><updated>2017-12-22T00:00:00+09:00</updated><id>http://localhost:4000/ML_note#RNN-1(%EB%AF%B8%EC%99%84%EC%84%B1)</id><content type="html" xml:base="http://localhost:4000/ML_note-RNN-1(%EB%AF%B8%EC%99%84%EC%84%B1)/">&lt;h1 id=&quot;cnn&quot;&gt;CNN&lt;/h1&gt;
&lt;blockquote&gt;
  &lt;p&gt;김성훈 교수님의 온라인 강의 및 논문 참고&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;개요&quot;&gt;개요&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;CNN이란
    &lt;ul&gt;
      &lt;li&gt;Convolutional Neural Network&lt;/li&gt;
      &lt;li&gt;Convolution : 합성곱, &lt;del&gt;대단히 복잡한 것(네이버사전)&lt;/del&gt;&lt;/li&gt;
      &lt;li&gt;NN을 이용한 이미지 인식 기술 중 현재 제일 좋은 성과를 보이고 있는 알고리즘&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;기존 알고리즘의 문제점
    &lt;ul&gt;
      &lt;li&gt;모든 픽셀을 다 교육하려니까 사진의 픽셀 수가 너무 많음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;기본 아이디어
    &lt;ul&gt;
      &lt;li&gt;사람의 시각적 인식은 수용영역과 추상화라는 개념을 통해서 간소화 됨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;수용영역
    &lt;ul&gt;
      &lt;li&gt;사람의 수용영역 = 사람의 초점&lt;/li&gt;
      &lt;li&gt;CNN의 수용영역 = 사진의 특정 부분(필터의 크기)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;추상화
    &lt;ul&gt;
      &lt;li&gt;사람의 추상화 = 사람의 얼굴을 보면 픽셀단위로 기억하지 않고 대충 그렇게 생긴 얼굴이라고 추상화하여 기억&lt;/li&gt;
      &lt;li&gt;CNN의 추상화 = 사진의 특정 부분 * 필터 (합성곱해서 나온 결과 사진, 뿌옇게 된 사진, 특징만 남은 사진)을 기억&lt;/li&gt;
      &lt;li&gt;필터 = 특징을 잡아내 주는 역활&lt;/li&gt;
      &lt;li&gt;모든 픽셀 데이터를 순간순간 마다 다 기억하지 않아도 됨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;용어-개념&quot;&gt;용어 개념&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;picture : 10 * 10 픽셀의 그림(sample)&lt;/li&gt;
  &lt;li&gt;filter : 3 * 3 행렬 (여러가지 형태가 나올 수 있음)&lt;/li&gt;
  &lt;li&gt;stride : 필터를 한번에 몇 칸씩 옮길지 (1로 가정)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;기존-nn으로의-해석&quot;&gt;기존 NN으로의 해석&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;그림에서 필터링 되는 각각의 부분들 = X&lt;/li&gt;
  &lt;li&gt;필터 = W&lt;/li&gt;
  &lt;li&gt;Y = WX 모델&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;전체-순서&quot;&gt;전체 순서&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;첫번쨰 필터셋(W1, W2, … , W6) 6개&lt;/li&gt;
  &lt;li&gt;W1필터로 왼쪽 위부터 순서대로 한 칸씩 옮기며 값 생성(W1X1, W1X2, W1X3, …)
«««&amp;lt; WX가 3차원 * 3차원인데?»»»»&lt;/li&gt;
  &lt;li&gt;100번 반복하여 10 * 10 행렬 생성(layer 하나 생성)&lt;/li&gt;
  &lt;li&gt;filter를 변경하여 또 다른 layer생성(W2X1, W2X2, W2X3, …)&lt;/li&gt;
  &lt;li&gt;만약 filter가 6개 라면 10 * 10 * 6(depth) 짜리 행렬 생성 -&amp;gt; 1차 추상화된 그림&lt;/li&gt;
  &lt;li&gt;추상화된 그림을 두번째 필터셋으로 교육&lt;/li&gt;
  &lt;li&gt;…&lt;/li&gt;
  &lt;li&gt;마지막 필터셋에는 relu대신 softmax함수 사용&lt;/li&gt;
  &lt;li&gt;«««&amp;lt; 마지막 추상화된 그림은 softmax로 한다는데 어떻게 한다는건지…»»»»&lt;/li&gt;
  &lt;li&gt;최종목적이 filter의 최적화?&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;추가&quot;&gt;추가&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;relu(WX)를 각각에 넣어주면 더 좋음&lt;/li&gt;
  &lt;li&gt;중간중간 pooling 해주는 것이 좋음&lt;/li&gt;
  &lt;li&gt;pooling = sampling = 그림을 n등분 하여 각 구역의 최고값을 추출해서 만듬&lt;/li&gt;
  &lt;li&gt;축소된 n * n 행렬이 나올것&lt;/li&gt;
  &lt;li&gt;아무튼 마지막에 나온 행렬을 softmax함수로 꺼내서 어느것인지 예측하고 update&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;직관적으로-이해하기&quot;&gt;직관적으로 이해하기&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;전체적 관점 : 그림을 필터링 -&amp;gt; 추상화된 그림&lt;/li&gt;
  &lt;li&gt;세부적 관점 : 그림의 각각의 조각을 필터링한 뒤 합친 그림 -&amp;gt; 추상화된 그림&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;중요한-점&quot;&gt;중요한 점&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;각 조각의 크기는 그대로 이나 depth가 계속 변화(filter가 몇개냐에 따라)&lt;/li&gt;
  &lt;li&gt;그림의 depth = filter의 depth(depth = 한번에 넣어주는 그림의 개수)&lt;/li&gt;
  &lt;li&gt;WX 보다 relu(WX)가 더 효과가 더 좋음&lt;/li&gt;
  &lt;li&gt;pooling 이용시 효과가 더 좋음 : 구역의 크기(n * n)을 정해서 그림을 등분한 뒤 각각의 구역의 최대값만을 추린 행렬(=sampling)&lt;/li&gt;
  &lt;li&gt;conv시 padding값을 주지 않으면 추상화된 그림은 작아짐&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;질문&quot;&gt;질문&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;그림 또는 필터의 depth는 어떤 의미가 있는가?&lt;/li&gt;
  &lt;li&gt;그림도 3차원이고 필터도 3차원인데 어떻게 행렬곱을 하는가?&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="Note" /><summary type="html">Machine Learning</summary></entry><entry><title type="html">ML_note #CNN-1</title><link href="http://localhost:4000/ML_note-CNN-1(%EB%AF%B8%EC%99%84%EC%84%B1)/" rel="alternate" type="text/html" title="ML_note #CNN-1" /><published>2017-12-22T00:00:00+09:00</published><updated>2017-12-22T00:00:00+09:00</updated><id>http://localhost:4000/ML_note#CNN-1(%EB%AF%B8%EC%99%84%EC%84%B1)</id><content type="html" xml:base="http://localhost:4000/ML_note-CNN-1(%EB%AF%B8%EC%99%84%EC%84%B1)/">&lt;h1 id=&quot;cnn&quot;&gt;CNN&lt;/h1&gt;
&lt;blockquote&gt;
  &lt;p&gt;김성훈 교수님의 온라인 강의 및 논문 참고&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;개요&quot;&gt;개요&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;CNN이란
    &lt;ul&gt;
      &lt;li&gt;Convolutional Neural Network&lt;/li&gt;
      &lt;li&gt;Convolution : 합성곱, &lt;del&gt;대단히 복잡한 것(네이버사전)&lt;/del&gt;&lt;/li&gt;
      &lt;li&gt;NN을 이용한 이미지 인식 기술 중 현재 제일 좋은 성과를 보이고 있는 알고리즘&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;기존 알고리즘의 문제점
    &lt;ul&gt;
      &lt;li&gt;모든 픽셀을 다 교육하려니까 사진의 픽셀 수가 너무 많음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;기본 아이디어
    &lt;ul&gt;
      &lt;li&gt;사람의 시각적 인식은 수용영역과 추상화라는 개념을 통해서 간소화 됨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;수용영역
    &lt;ul&gt;
      &lt;li&gt;사람의 수용영역 = 사람의 초점&lt;/li&gt;
      &lt;li&gt;CNN의 수용영역 = 사진의 특정 부분(필터의 크기)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;추상화
    &lt;ul&gt;
      &lt;li&gt;사람의 추상화 = 사람의 얼굴을 보면 픽셀단위로 기억하지 않고 대충 그렇게 생긴 얼굴이라고 추상화하여 기억&lt;/li&gt;
      &lt;li&gt;CNN의 추상화 = 사진의 특정 부분 * 필터 (합성곱해서 나온 결과 사진, 뿌옇게 된 사진, 특징만 남은 사진)을 기억&lt;/li&gt;
      &lt;li&gt;필터 = 특징을 잡아내 주는 역활&lt;/li&gt;
      &lt;li&gt;모든 픽셀 데이터를 순간순간 마다 다 기억하지 않아도 됨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;용어-개념&quot;&gt;용어 개념&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;picture : 10 * 10 픽셀의 그림(sample)&lt;/li&gt;
  &lt;li&gt;filter : 3 * 3 행렬 (여러가지 형태가 나올 수 있음)&lt;/li&gt;
  &lt;li&gt;stride : 필터를 한번에 몇 칸씩 옮길지 (1로 가정)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;기존-nn으로의-해석&quot;&gt;기존 NN으로의 해석&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;그림에서 필터링 되는 각각의 부분들 = X&lt;/li&gt;
  &lt;li&gt;필터 = W&lt;/li&gt;
  &lt;li&gt;Y = WX 모델&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;전체-순서&quot;&gt;전체 순서&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;첫번쨰 필터셋(W1, W2, … , W6) 6개&lt;/li&gt;
  &lt;li&gt;W1필터로 왼쪽 위부터 순서대로 한 칸씩 옮기며 값 생성(W1X1, W1X2, W1X3, …)
«««&amp;lt; WX가 3차원 * 3차원인데?»»»»&lt;/li&gt;
  &lt;li&gt;100번 반복하여 10 * 10 행렬 생성(layer 하나 생성)&lt;/li&gt;
  &lt;li&gt;filter를 변경하여 또 다른 layer생성(W2X1, W2X2, W2X3, …)&lt;/li&gt;
  &lt;li&gt;만약 filter가 6개 라면 10 * 10 * 6(depth) 짜리 행렬 생성 -&amp;gt; 1차 추상화된 그림&lt;/li&gt;
  &lt;li&gt;추상화된 그림을 두번째 필터셋으로 교육&lt;/li&gt;
  &lt;li&gt;…&lt;/li&gt;
  &lt;li&gt;마지막 필터셋에는 relu대신 softmax함수 사용&lt;/li&gt;
  &lt;li&gt;«««&amp;lt; 마지막 추상화된 그림은 softmax로 한다는데 어떻게 한다는건지…»»»»&lt;/li&gt;
  &lt;li&gt;최종목적이 filter의 최적화?&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;추가&quot;&gt;추가&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;relu(WX)를 각각에 넣어주면 더 좋음&lt;/li&gt;
  &lt;li&gt;중간중간 pooling 해주는 것이 좋음&lt;/li&gt;
  &lt;li&gt;pooling = sampling = 그림을 n등분 하여 각 구역의 최고값을 추출해서 만듬&lt;/li&gt;
  &lt;li&gt;축소된 n * n 행렬이 나올것&lt;/li&gt;
  &lt;li&gt;아무튼 마지막에 나온 행렬을 softmax함수로 꺼내서 어느것인지 예측하고 update&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;직관적으로-이해하기&quot;&gt;직관적으로 이해하기&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;전체적 관점 : 그림을 필터링 -&amp;gt; 추상화된 그림&lt;/li&gt;
  &lt;li&gt;세부적 관점 : 그림의 각각의 조각을 필터링한 뒤 합친 그림 -&amp;gt; 추상화된 그림&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;중요한-점&quot;&gt;중요한 점&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;각 조각의 크기는 그대로 이나 depth가 계속 변화(filter가 몇개냐에 따라)&lt;/li&gt;
  &lt;li&gt;그림의 depth = filter의 depth(depth = 한번에 넣어주는 그림의 개수)&lt;/li&gt;
  &lt;li&gt;WX 보다 relu(WX)가 더 효과가 더 좋음&lt;/li&gt;
  &lt;li&gt;pooling 이용시 효과가 더 좋음 : 구역의 크기(n * n)을 정해서 그림을 등분한 뒤 각각의 구역의 최대값만을 추린 행렬(=sampling)&lt;/li&gt;
  &lt;li&gt;conv시 padding값을 주지 않으면 추상화된 그림은 작아짐&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;질문&quot;&gt;질문&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;그림 또는 필터의 depth는 어떤 의미가 있는가?&lt;/li&gt;
  &lt;li&gt;그림도 3차원이고 필터도 3차원인데 어떻게 행렬곱을 하는가?&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="Note" /><summary type="html">Machine Learning</summary></entry><entry><title type="html">ML_note #Linear Regression-1(미완성)</title><link href="http://localhost:4000/ML_note-Linear-Regression-1(%EB%AF%B8%EC%99%84%EC%84%B1)/" rel="alternate" type="text/html" title="ML_note #Linear Regression-1(미완성)" /><published>2017-12-22T00:00:00+09:00</published><updated>2017-12-22T00:00:00+09:00</updated><id>http://localhost:4000/ML_note#Linear%20Regression-1(%EB%AF%B8%EC%99%84%EC%84%B1)</id><content type="html" xml:base="http://localhost:4000/ML_note-Linear-Regression-1(%EB%AF%B8%EC%99%84%EC%84%B1)/">&lt;h1 id=&quot;linear-regression선형회귀&quot;&gt;Linear Regression(선형회귀)&lt;/h1&gt;
&lt;h4 id=&quot;개요&quot;&gt;개요&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;교사학습의 가장 기본적인 모델&lt;/li&gt;
  &lt;li&gt;문제와 정답을 통해서 학습&lt;/li&gt;
  &lt;li&gt;어떤 선형그래프를 그려내는지 추론하는 것(y = ax+b)&lt;/li&gt;
  &lt;li&gt;x(입력)에 대한 y(정답)을 주어주고 가중치(a)와 bias(b)를 추론하는 것&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;기본문제&quot;&gt;기본문제&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;학습
    &lt;ul&gt;
      &lt;li&gt;수학시험에 50점(입력)을 받으면 C(정답) 였다&lt;/li&gt;
      &lt;li&gt;수학시험에 70점(입력)을 받으면 B(정답) 였다&lt;/li&gt;
      &lt;li&gt;수학시험에 90점(입력)을 받으면 A(정답) 였다&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;학습을 통해 근사되어갈 그래프
    &lt;ul&gt;
      &lt;li&gt;A = 4, B = 3, C = 2 이라고 할 떄( (+)점수는 0.5 )&lt;/li&gt;
      &lt;li&gt;y = 0.05 * x - 0.5 (a = 0.05, b = -0.5)로 근사될 것&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;예측
    &lt;ul&gt;
      &lt;li&gt;만약 80점을 맞으면 내 학점은 어떻게 될 것인가? B+(정답)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;학습은-어떻게&quot;&gt;학습은 어떻게?&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Cost&lt;/li&gt;
  &lt;li&gt;Regression&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;모델&quot;&gt;모델&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;정식표기 : Y = WX + b&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;sigmoid&quot;&gt;sigmoid&lt;/h4&gt;
&lt;h4 id=&quot;relu&quot;&gt;relu&lt;/h4&gt;
&lt;h4 id=&quot;softmax&quot;&gt;softmax&lt;/h4&gt;</content><author><name></name></author><category term="Note" /><summary type="html">Machine Learning</summary></entry></feed>